# Adding Paper Results

The existing paper benchmark results on sotabench come from the 
[Papers With Code](http://www.paperswithcode.com) resource. There may be cases where you are
evaluating a model whose corresponding paper does not yet have
results on Papers With Code (and sotabench as well).

You can add paper results by specifying a dictionary in the ```paper_results``` argument of the
benchmark method. For example, below we have added new paper results for a 'MyNet' model with
Top 1 Accuracy of 75.4% and Top 5 Accuracy of 85.65%:

    from torchbench.image_classification import ImageNet
    from torchvision.models.mynet import mynet101
    import torchvision.transforms as transforms
    import PIL
    
    # Define the transforms need to convert ImageNet data to expected model input
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    input_transform = transforms.Compose([
        transforms.Resize(256, PIL.Image.BICUBIC),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        normalize,
    ])
    
    mynet_paper_results = {'Top 1 Accuracy': 0.754, 'Top 5 Accuracy': 0.8565}
    
    # Run the benchmark
    ImageNet.benchmark(
        model=mynet101(pretrained=True),
        paper_model_name='MyNet',
        paper_arxiv_id='2099.05431',
        paper_results=mynet_paper_results,
        input_transform=input_transform,
        batch_size=256,
        num_gpu=1
    )

Make sure that the metric names match those on the existing benchmark page on 
[sotabench](http://www.sotabench.com) if you want comparable results to other models. For
example, ImageNet has the metrics 'Top 1 Accuracy' and 'Top 5 Accuracy'.